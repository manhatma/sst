package psst

import (
	"bufio"
	"errors"
	"fmt"
	"math"
	"strings"

	"github.com/SeanJxie/polygo"
	"github.com/google/uuid"
	"github.com/openacid/slimarray/polyfit"
	"gonum.org/v1/gonum/floats"
	"golang.org/x/exp/constraints"
)

const (
	VELOCITY_ZERO_THRESHOLD             = 0.02
	IDLING_DURATION_THRESHOLD           = 0.01
	AIRTIME_TRAVEL_THRESHOLD            = 3
	AIRTIME_DURATION_THRESHOLD          = 0.20
	AIRTIME_VELOCITY_THRESHOLD          = 500
	AIRTIME_OVERLAP_THRESHOLD           = 0.5
	AIRTIME_TRAVEL_MEAN_THRESHOLD_RATIO = 0.04
	STROKE_LENGTH_THRESHOLD             = 5
	TRAVEL_HIST_BINS                    = 40
	VELOCITY_HIST_TRAVEL_BINS           = 10
	VELOCITY_HIST_STEP                  = 100.0
	VELOCITY_HIST_STEP_FINE             = 15.0
	BOTTOMOUT_THRESHOLD                 = 3

	// Parameters for Modified Sinc Smoother:
	// Based on Schmid, M., Rath, D., & Diebold, U. (2022).
	// Why and How Savitzky-Golay Filters Should Be Replaced. ACS Measurement Science Au, 2, 185-196. [cite: 1]
	// MS_N_DEGREE corresponds to 'n' in the paper, determining kernel shape. [cite: 1]
	// MS_N_DEGREE = 2 is chosen for a low-frequency response shape similar to
	// what WH_ORDER = 2 would aim for (see paper's Eq. 13: p = 1 + n/2). [cite: 1]
	MS_N_DEGREE = 4
	// MS_ALPHA is the Gaussian width parameter for the MS window function. [cite: 1]
	MS_ALPHA = 4.0
	// MS_M_VALUE is the half-width 'm' of the MS kernel. It controls smoothness. [cite: 1]
	// Larger m = more smoothing, smaller cutoff frequency.
	// Minimum required m for MS_N_DEGREE=2 is n/2+2 = 3 (see paper's Table 2 caption). [cite: 1]
	// Setting MS_M_VALUE below this minimum (e.g., 0) will skip smoothing,
	// effectively mimicking no smoothing (like WH_LAMBDA = 0 previously might have intended).
	// To enable smoothing, set MS_M_VALUE to 3 or higher (e.g., 3 for minimal, 15 for moderate).
	MS_M_VALUE = 5 // Default to no smoothing. Change to >=3 to enable.
)

// calculateDerivative computes the derivative using central differences for inner points
// and forward/backward differences for boundary points.
func calculateDerivative(data []float64, sampleRate uint16) ([]float64, error) {
	n := len(data)
	if n == 0 {
		return []float64{}, nil
	}
	if sampleRate == 0 {
		return nil, errors.New("SampleRate must not be zero for derivative calculation")
	}
	if n < 2 && n > 0 { // Handles n=1 case
		derivative := make([]float64, n)
		derivative[0] = 0
		return derivative, nil
	}
	if n < 1 { // Should already be covered by n==0
		return []float64{}, nil
	}

	dt := 1.0 / float64(sampleRate)
	derivative := make([]float64, n)

	// Ensure data[1] exists for forward difference at the start
	if n > 1 {
		derivative[0] = (data[1] - data[0]) / dt
	} else { // Only one point, derivative is 0
		derivative[0] = 0
		return derivative, nil // Early exit as no further calculations are possible
	}

	for i := 1; i < n-1; i++ {
		derivative[i] = (data[i+1] - data[i-1]) / (2 * dt)
	}
	// Ensure data[n-2] exists for backward difference at the end (n > 1 already checked)
	derivative[n-1] = (data[n-1] - data[n-2]) / dt

	return derivative, nil
}

type LinkageRecord struct {
	ShockTravel   float64
	WheelTravel   float64
	LeverageRatio float64
}

type Linkage struct {
	Id               uuid.UUID    `codec:"-" db:"id"           json:"id"`
	Name             string       `codec:"," db:"name"         json:"name"         binding:"required"`
	HeadAngle        float64      `codec:"," db:"head_angle"   json:"head_angle"   binding:"required"`
	RawData          string       `codec:"-" db:"raw_lr_data"  json:"data"         binding:"required"`
	MaxFrontStroke   float64      `codec:"," db:"front_stroke" json:"front_stroke" binding:"required"`
	MaxRearStroke    float64      `codec:"," db:"rear_stroke"  json:"rear_stroke"  binding:"required"`
	MaxFrontTravel   float64      `codec:","                   json:"-"`
	MaxRearTravel    float64      `codec:","                   json:"-"`
	LeverageRatio    [][2]float64 `codec:","                   json:"-"`
	ShockWheelCoeffs []float64    `codec:","                   json:"-"`
	polynomial       *polygo.RealPolynomial
}

type suspension struct {
	Present                bool
	Calibration            Calibration // Defined in calibration.go
	Travel                 []float64
	Velocity               []float64
	Strokes                strokes     // Defined in stroke.go
	TravelBins             []float64
	VelocityBins           []float64
	FineVelocityBins       []float64
	GlobalMaxTravelAllData float64
	GlobalP95TravelAllData float64
	GlobalAvgTravelAllData float64
}

type Number interface {
	constraints.Float | constraints.Integer
}

type Meta struct {
	Name       string
	Version    uint8
	SampleRate uint16
	Timestamp  int64
}

type SetupData struct {
	Linkage          *Linkage
	FrontCalibration *Calibration // Defined in calibration.go
	RearCalibration  *Calibration // Defined in calibration.go
}

type Processed struct {
	Meta
	Front    suspension
	Rear     suspension
	Linkage  Linkage
	Airtimes []*airtime // airtime defined in stroke.go, airtimes method in airtimes.go
}

func (this *Linkage) ProcessRawData() error {
	var records []LinkageRecord
	scanner := bufio.NewScanner(strings.NewReader(this.RawData))
	s := 0.0 // shock travel
	for scanner.Scan() {
		var w, l float64 // wheel travel, leverage ratio
		_, err := fmt.Sscanf(scanner.Text(), "%f,%f", &w, &l)
		if err == nil {
			records = append(records, LinkageRecord{
				ShockTravel:   s,
				WheelTravel:   w,
				LeverageRatio: l,
			})
			if l != 0 { // Avoid division by zero if leverage ratio is zero
				s += 1.0 / l
			} else {
				// Handle zero leverage ratio appropriately, e.g., increment by a small default or log error
				// For now, just don't increment s to prevent NaN/Inf
			}
		}
	}
	this.Process(records)
	return nil
}

func (this *Linkage) Process(records []LinkageRecord) {
	var st []float64 // shock_travel
	var wt []float64 // wheel_travel
	var wtlr [][2]float64 // wheel_travel_leverage_ratio pairs

	for _, record := range records {
		st = append(st, record.ShockTravel)
		wt = append(wt, record.WheelTravel)
		wtlr = append(wtlr, [2]float64{record.WheelTravel, record.LeverageRatio})
	}

	f := polyfit.NewFit(st, wt, 3) // Fit a 3rd degree polynomial
	this.LeverageRatio = wtlr
	this.ShockWheelCoeffs = f.Solve()
	this.polynomial, _ = polygo.NewRealPolynomial(this.ShockWheelCoeffs)
	this.MaxRearTravel = this.polynomial.At(this.MaxRearStroke)
	this.MaxFrontTravel = math.Sin(this.HeadAngle*math.Pi/180.0) * this.MaxFrontStroke
}

func linspace(min, max float64, num int) []float64 {
	if num <= 0 {
		return []float64{}
	}
	if num == 1 {
		return []float64{min}
	}
	step := (max - min) / float64(num-1)
	bins := make([]float64, num)
	for i := range bins {
		bins[i] = min + step*float64(i)
	}
	return bins
}

type MissingRecordsError struct{}

func (e *MissingRecordsError) Error() string { return "Front and Rear record arrays are empty" }

// RecordCountMismatchError is not explicitly used in the current logic with MS smoothing,
// as Front and Rear can be smoothed independently.
// type RecordCountMismatchError struct{}
// func (e *RecordCountMismatchError) Error() string { return "Number of Front and Rear records is not equal" }

func ProcessRecording[T Number](front, rear []T, meta Meta, setup *SetupData) (*Processed, error) {
	// Minimum m value required for the currently configured MS_N_DEGREE
	minRequiredMsMValue := MS_N_DEGREE/2 + 2

	var pd Processed
	pd.Meta = meta
	// Calibration is assigned from setup.FrontCalibration, its type comes from calibration.go
	pd.Front.Calibration = *setup.FrontCalibration
	pd.Rear.Calibration = *setup.RearCalibration
	pd.Linkage = *setup.Linkage

	fc := len(front)
	rc := len(rear)
	pd.Front.Present = fc != 0
	pd.Rear.Present = rc != 0

	if !(pd.Front.Present || pd.Rear.Present) {
		return nil, &MissingRecordsError{}
	}

	if pd.Front.Present {
		pd.Front.Travel = make([]float64, fc)
		front_coeff := math.Sin(pd.Linkage.HeadAngle * math.Pi / 180.0)
		for idx, value := range front {
			out, _ := pd.Front.Calibration.Evaluate(float64(value)) // Evaluate from calibration.go
			x := out * front_coeff
			x = math.Max(0, x)
			x = math.Min(x, pd.Linkage.MaxFrontTravel)
			pd.Front.Travel[idx] = x
		}

		if len(pd.Front.Travel) > 0 {
			pd.Front.GlobalMaxTravelAllData = floats.Max(pd.Front.Travel)
			pd.Front.GlobalP95TravelAllData = getPercentileValue(pd.Front.Travel, 0.95) // from stroke.go
			pd.Front.GlobalAvgTravelAllData = floats.Sum(pd.Front.Travel) / float64(len(pd.Front.Travel))
		}

		var dtFront []int
		if pd.Linkage.MaxFrontTravel > 0 {
			tbins := linspace(0, pd.Linkage.MaxFrontTravel, TRAVEL_HIST_BINS+1)
			dtFront = digitize(pd.Front.Travel, tbins) // from stroke.go
			pd.Front.TravelBins = tbins
		} else {
			pd.Front.TravelBins = []float64{}
			dtFront = make([]int, fc)
		}
		// pd.Front.Strokes.digitizeTravel(dtFront) // Obsolete call

		if MS_M_VALUE >= minRequiredMsMValue && pd.Meta.SampleRate > 0 && fc > 0 {
			msSmootherFront, errMs := NewModifiedSincSmoother(MS_N_DEGREE, MS_M_VALUE, MS_ALPHA)
			if errMs == nil {
				smoothedTravel, errSmooth := msSmootherFront.Smooth(pd.Front.Travel)
				if errSmooth == nil {
					velocity, errVel := calculateDerivative(smoothedTravel, pd.Meta.SampleRate)
					if errVel == nil {
						pd.Front.Velocity = velocity
					} else {
						fmt.Printf("Warning: Error calculating front velocity after MS smoothing: %v. Zero velocity will be used.\n", errVel)
						pd.Front.Velocity = make([]float64, fc)
					}
				} else {
					fmt.Printf("Warning: Error MS-smoothing front travel: %v. Zero velocity will be used.\n", errSmooth)
					pd.Front.Velocity = make([]float64, fc)
				}
			} else {
				fmt.Printf("Warning: Error creating front MS-smoother: %v. Zero velocity will be used.\n", errMs)
				pd.Front.Velocity = make([]float64, fc)
			}
		} else {
			if MS_M_VALUE < minRequiredMsMValue {
				fmt.Printf("Notice: MS_M_VALUE (%d) < %d for Front, MS smoothing skipped. Velocity calculated from original data.\n", MS_M_VALUE, minRequiredMsMValue)
			}
			// Calculate velocity from original (unsmoothed) travel data
			if pd.Meta.SampleRate > 0 && fc > 0 {
				velocity, errVel := calculateDerivative(pd.Front.Travel, pd.Meta.SampleRate)
				if errVel == nil {
					pd.Front.Velocity = velocity
				} else {
					fmt.Printf("Warning: Error calculating front velocity (unsmoothed): %v. Zero velocity will be used.\n", errVel)
					pd.Front.Velocity = make([]float64, fc)
				}
			} else {
				if pd.Meta.SampleRate == 0 { fmt.Printf("Warning: Front SampleRate is zero. Zero velocity will be used.\n") }
				if fc == 0 { fmt.Printf("Warning: No front data points. Zero velocity will be used.\n") }
				pd.Front.Velocity = make([]float64, fc)
			}
		}

		vbins, dv := digitizeVelocity(pd.Front.Velocity, VELOCITY_HIST_STEP) // from stroke.go
		pd.Front.VelocityBins = vbins
		vbinsFine, dvFine := digitizeVelocity(pd.Front.Velocity, VELOCITY_HIST_STEP_FINE) // from stroke.go
		pd.Front.FineVelocityBins = vbinsFine

		currentStrokes := filterStrokes(pd.Front.Velocity, pd.Front.Travel, pd.Linkage.MaxFrontTravel, pd.Meta.SampleRate) // from stroke.go
		pd.Front.Strokes.categorize(currentStrokes, pd.Front.Travel, pd.Linkage.MaxFrontTravel) // from stroke.go

		if len(pd.Front.Strokes.Compressions) == 0 && len(pd.Front.Strokes.Rebounds) == 0 {
			pd.Front.Present = false
		} else {
			// pd.Front.Strokes.digitizeVelocity(dv, dvFine) // Obsolete call
			pd.Front.Strokes.digitize(dtFront, dv, dvFine) // Corrected call according to stroke.go
		}
	}

	if pd.Rear.Present {
		pd.Rear.Travel = make([]float64, rc)
		for idx, value := range rear {
			out, _ := pd.Rear.Calibration.Evaluate(float64(value)) // from calibration.go
			x := pd.Linkage.polynomial.At(out)
			x = math.Max(0, x)
			x = math.Min(x, pd.Linkage.MaxRearTravel)
			pd.Rear.Travel[idx] = x
		}

		if len(pd.Rear.Travel) > 0 {
			pd.Rear.GlobalMaxTravelAllData = floats.Max(pd.Rear.Travel)
			pd.Rear.GlobalP95TravelAllData = getPercentileValue(pd.Rear.Travel, 0.95) // from stroke.go
			pd.Rear.GlobalAvgTravelAllData = floats.Sum(pd.Rear.Travel) / float64(len(pd.Rear.Travel))
		}

		var dtRear []int
		if pd.Linkage.MaxRearTravel > 0 {
			tbins := linspace(0, pd.Linkage.MaxRearTravel, TRAVEL_HIST_BINS+1)
			dtRear = digitize(pd.Rear.Travel, tbins) // from stroke.go
			pd.Rear.TravelBins = tbins
		} else {
			pd.Rear.TravelBins = []float64{}
			dtRear = make([]int, rc)
		}
		// pd.Rear.Strokes.digitizeTravel(dtRear) // Obsolete call

		if MS_M_VALUE >= minRequiredMsMValue && pd.Meta.SampleRate > 0 && rc > 0 {
			msSmootherRear, errMs := NewModifiedSincSmoother(MS_N_DEGREE, MS_M_VALUE, MS_ALPHA)
			if errMs == nil {
				smoothedTravel, errSmooth := msSmootherRear.Smooth(pd.Rear.Travel)
				if errSmooth == nil {
					velocity, errVel := calculateDerivative(smoothedTravel, pd.Meta.SampleRate)
					if errVel == nil {
						pd.Rear.Velocity = velocity
					} else {
						fmt.Printf("Warning: Error calculating rear velocity after MS smoothing: %v. Zero velocity will be used.\n", errVel)
						pd.Rear.Velocity = make([]float64, rc)
					}
				} else {
					fmt.Printf("Warning: Error MS-smoothing rear travel: %v. Zero velocity will be used.\n", errSmooth)
					pd.Rear.Velocity = make([]float64, rc)
				}
			} else {
				fmt.Printf("Warning: Error creating rear MS-smoother: %v. Zero velocity will be used.\n", errMs)
				pd.Rear.Velocity = make([]float64, rc)
			}
		} else {
			if MS_M_VALUE < minRequiredMsMValue {
				fmt.Printf("Notice: MS_M_VALUE (%d) < %d for Rear, MS smoothing skipped. Velocity calculated from original data.\n", MS_M_VALUE, minRequiredMsMValue)
			}
			// Calculate velocity from original (unsmoothed) travel data
			if pd.Meta.SampleRate > 0 && rc > 0 {
				velocity, errVel := calculateDerivative(pd.Rear.Travel, pd.Meta.SampleRate)
				if errVel == nil {
					pd.Rear.Velocity = velocity
				} else {
					fmt.Printf("Warning: Error calculating rear velocity (unsmoothed): %v. Zero velocity will be used.\n", errVel)
					pd.Rear.Velocity = make([]float64, rc)
				}
			} else {
				if pd.Meta.SampleRate == 0 { fmt.Printf("Warning: Rear SampleRate is zero. Zero velocity will be used.\n") }
				if rc == 0 { fmt.Printf("Warning: No rear data points. Zero velocity will be used.\n") }
				pd.Rear.Velocity = make([]float64, rc)
			}
		}

		vbins, dv := digitizeVelocity(pd.Rear.Velocity, VELOCITY_HIST_STEP) // from stroke.go
		pd.Rear.VelocityBins = vbins
		vbinsFine, dvFine := digitizeVelocity(pd.Rear.Velocity, VELOCITY_HIST_STEP_FINE) // from stroke.go
		pd.Rear.FineVelocityBins = vbinsFine

		currentStrokes := filterStrokes(pd.Rear.Velocity, pd.Rear.Travel, pd.Linkage.MaxRearTravel, pd.Meta.SampleRate) // from stroke.go
		pd.Rear.Strokes.categorize(currentStrokes, pd.Rear.Travel, pd.Linkage.MaxRearTravel) // from stroke.go
		if len(pd.Rear.Strokes.Compressions) == 0 && len(pd.Rear.Strokes.Rebounds) == 0 {
			pd.Rear.Present = false
		} else {
			// pd.Rear.Strokes.digitizeVelocity(dv, dvFine) // Obsolete call
			pd.Rear.Strokes.digitize(dtRear, dv, dvFine) // Corrected call according to stroke.go
		}
	}

	pd.airtimes() // Method from airtimes.go
	return &pd, nil
}